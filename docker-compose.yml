version: "3.9"

services:
  titanic-api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: titanic-api
    ports:
      - "8000:8000"
    volumes:
      - .:/app
    command: uvicorn ml.predict_api:app --host 0.0.0.0 --port 8000 --reload

  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.11.1
    container_name: mlflow-ui
    ports:
      - "5000:5000"
    volumes:
      - ./mlruns:/mlflow/mlruns
    command: >
      mlflow server 
      --backend-store-uri /mlflow/mlruns 
      --default-artifact-root /mlflow/mlruns 
      --host 0.0.0.0
